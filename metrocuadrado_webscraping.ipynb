{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metrocuadrado_webscraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGa3+2RAUvVxVNa2zeMPJ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juapatral/webscrapers/blob/main/metrocuadrado_webscraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQSDwLSbTtbZ",
        "outputId": "d72f7806-c7f7-477a-94ba-1cb3037710b1"
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [635 kB]\n",
            "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,751 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [372 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [24.5 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,077 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,176 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,404 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [896 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [402 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,508 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [31.4 kB]\n",
            "Get:27 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [39.5 kB]\n",
            "Fetched 12.6 MB in 4s (3,508 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 46 not upgraded.\n",
            "Need to get 83.2 MB of archives.\n",
            "After this operation, 282 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 89.0.4389.90-0ubuntu0.18.04.2 [1,127 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 89.0.4389.90-0ubuntu0.18.04.2 [73.6 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 89.0.4389.90-0ubuntu0.18.04.2 [3,809 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 89.0.4389.90-0ubuntu0.18.04.2 [4,697 kB]\n",
            "Fetched 83.2 MB in 4s (21.8 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 160983 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_89.0.4389.90-0ubuntu0.18.04.2_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_89.0.4389.90-0ubuntu0.18.04.2_amd64.deb ...\n",
            "Unpacking chromium-browser (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_89.0.4389.90-0ubuntu0.18.04.2_all.deb ...\n",
            "Unpacking chromium-browser-l10n (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_89.0.4389.90-0ubuntu0.18.04.2_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Setting up chromium-browser (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Setting up chromium-browser-l10n (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rA6VD1vTL0A",
        "outputId": "da3b39c0-1802-4647-c354-8c3ac932084e"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "print(\n",
        "\"\"\"\n",
        "#-------------------------------------------------------------------------\n",
        "# Author: Juan Pablo Trujillo Alviz \n",
        "# github: juapatral\n",
        "# CD: 2021-04-07 \n",
        "# LUD: 2021-04-07\n",
        "# Description: webscrapping of metrocuadrado website\n",
        "# Use: \n",
        "#\n",
        "#\n",
        "# v1\n",
        "# Modification:\n",
        "# Description:\n",
        "#-------------------------------------------------------------------------\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# importar librerias\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import glob\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "# funciones propias\n",
        "\n",
        "## extraer detalle informacion\n",
        "def extraer_detalle_mc(url):\n",
        "\n",
        "    # obtener driver\n",
        "    driver.get(url)\n",
        "\n",
        "    # obtener codigo fuente\n",
        "    try:\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "    # obtener script pagina\n",
        "    script = soup.find_all(\"script\", {\"type\":\"application/json\"})\n",
        "\n",
        "    if script == []:\n",
        "        return None\n",
        "    \n",
        "    # find json\n",
        "    try:\n",
        "        my_json = json.loads(re.findall(\"\\\"realEstate\\\":(.*})}}}.*$\", script[0].text)[0])\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "    # convert to df\n",
        "    return pd.json_normalize(my_json)\n",
        "\n",
        "## avanzar pagina\n",
        "def avance_pagina(driver):\n",
        "\n",
        "    # obtener vinuclo del siguiente\n",
        "    element = driver.find_element_by_link_text(\"Next\")\n",
        "\n",
        "    # crear accion\n",
        "    actions = ActionChains(driver)\n",
        "\n",
        "    # moverse hasta el punto\n",
        "    actions.move_to_element(element).send_keys(Keys.SPACE).perform()\n",
        "\n",
        "    # dar click\n",
        "    element.click()\n",
        "\n",
        "    return None\n",
        "\n",
        "## particionar lista\n",
        "def particiones(numb, part=50000):\n",
        "    return numb//part + (1 if numb%part != 0 else 0)\n",
        "\n",
        "# parte 1\n",
        "## configurar opciones del chromedrive\n",
        "print(\"---------------Inicia parte 1: configuración del chrome---------------\")\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless') # habilitar en colab\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "#chromedriver = r\"D:\\jutrujil\\Datos\\gobierno_informacion_sia\\webscraping\\driver_v89\\chromedriver.exe\"\n",
        "chromedriver = \"chromedriver\"\n",
        "driver = webdriver.Chrome(chromedriver, chrome_options=chrome_options)\n",
        "driver.implicitly_wait(2)\n",
        "\n",
        "## paginas a buscar\n",
        "metro_urls = [\n",
        "    \"https://www.metrocuadrado.com/local/arriendo/\",\n",
        "    \"https://www.metrocuadrado.com/local/venta/\",\n",
        "    \"https://www.metrocuadrado.com/oficinas/arriendo/\",\n",
        "    \"https://www.metrocuadrado.com/oficinas/venta/\",\n",
        "    \"https://www.metrocuadrado.com/bodega/arriendo/\",\n",
        "    \"https://www.metrocuadrado.com/bodega/venta/\",\n",
        "    \"https://www.metrocuadrado.com/edificio-de-oficinas/arriendo/\",\n",
        "    \"https://www.metrocuadrado.com/edificio-de-oficinas/venta/\",\n",
        "    \"https://www.metrocuadrado.com/consultorio/arriendo/\",\n",
        "    \"https://www.metrocuadrado.com/consultorio/venta/\",\n",
        "    \"https://www.metrocuadrado.com/apartamento/arriendo/\",\n",
        "    \"https://www.metrocuadrado.com/apartamento/venta/\",\n",
        "    \"https://www.metrocuadrado.com/casa/arriendo/\",\n",
        "    \"https://www.metrocuadrado.com/casa/venta/\",\n",
        "    \"https://www.metrocuadrado.com/lote/arriendo/\",\n",
        "    \"https://www.metrocuadrado.com/lote/venta/\",\n",
        "    \"https://www.metrocuadrado.com/finca/arriendo/\",\n",
        "    \"https://www.metrocuadrado.com/finca/venta/\",\n",
        "    \"https://www.metrocuadrado.com/edificio-de-apartamentos/arriendo/\",\n",
        "    \"https://www.metrocuadrado.com/edificio-de-apartamentos/venta/\"\n",
        "]\n",
        "\n",
        "## alojar las urls de los inmuebles\n",
        "urls = []\n",
        "\n",
        "## definir fecha de inicio\n",
        "now = dt.datetime.now()\n",
        "fecha = now.year * 10000 + now.month * 100 + now.day\n",
        "\n",
        "print(\"Inicia la extracción a las: {}\".format(now))\n",
        "\n",
        "# parte 2\n",
        "## buscar los inmuebles por cada pagina\n",
        "print(\"---------------Inicia parte 2: búsqueda de inmuebles---------------\")\n",
        "\n",
        "urls_df_list = glob.glob(\"./urls_*.xlsx\")\n",
        "\n",
        "## revisar exista ya un archivo de urls\n",
        "if urls_df_list == []:\n",
        "    for metro_url in metro_urls:\n",
        "        print(\"Extracción de: \" + metro_url)\n",
        "        \n",
        "        # cargar la pagina\n",
        "        driver.get(metro_url)\n",
        "\n",
        "        # dar click en aceptar cookies\n",
        "        try:\n",
        "            driver.find_element_by_link_text(\"Acepto\").click()\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # convertir a html\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "        \n",
        "        # encontrar la cantidad de inmuebles\n",
        "        for j in range(3):\n",
        "            try:\n",
        "                r = soup.find_all(\"h1\", {\"class\":\"H1-xsrgru-0 jdfXCo d-sm-inline-block breadcrumb-item active\"})\n",
        "                break\n",
        "            except:\n",
        "                time.sleep(1)\n",
        "                j+=1\n",
        "                if j>=2:\n",
        "                    driver.refresh()\n",
        "        \n",
        "        # definir el numero de paginas a buscar\n",
        "        try:\n",
        "            paginas = min([int(re.findall(\"([0-9]+)\", r[0].text)[0])//50, 200])\n",
        "        except:\n",
        "            time.sleep(1)\n",
        "            driver.refresh()\n",
        "            continue\n",
        "\n",
        "        # contador de paginas\n",
        "        contador = 1\n",
        "\n",
        "        # iniciar extraccion por cada pagina\n",
        "        for i in range(paginas):\n",
        "            \n",
        "            # contador\n",
        "            contador += 1\n",
        "\n",
        "            # romper ciclo\n",
        "            if contador >= paginas*1.2//1:\n",
        "                break\n",
        "            \n",
        "            # convertir a html\n",
        "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "            # encontrar todos los inmuebles\n",
        "            objetos = soup.find_all(\"a\", {\"class\":\"sc-bdVaJa ebNrSm\"})\n",
        "            \n",
        "            # revisar objetos\n",
        "            if objetos != []:\n",
        "\n",
        "                # por cada inmueble\n",
        "                for objeto in objetos:\n",
        "                    try:\n",
        "                        urls.append(\"https://www.metrocuadrado.com\" + objeto[\"href\"])\n",
        "                    except:\n",
        "                        pass\n",
        "            \n",
        "            for j in range(3):\n",
        "                try:\n",
        "                    avance_pagina(driver)\n",
        "                    break\n",
        "                except:\n",
        "                    time.sleep(1)\n",
        "                    j+=1\n",
        "                    if j>=2:\n",
        "                        driver.refresh()\n",
        "                \n",
        "            time.sleep(1)\n",
        "    \n",
        "    ## crear archivo de urls\n",
        "    urls_file = \"./urls_metrocuadrado-{}.xlsx\".format(fecha)\n",
        "\n",
        "    ## guardar urls\n",
        "    urls_df = pd.DataFrame(urls)\n",
        "    with pd.ExcelWriter(urls_file, options={'strings_to_urls': False}) as file:\n",
        "        urls_df.to_excel(file, index=False)\n",
        "\n",
        "else:\n",
        "    urls_df = pd.read_excel(urls_df_list[0])\n",
        "    try: \n",
        "        urls = [*urls_df[0]]\n",
        "    except:\n",
        "        urls = [*urls_df[\"0\"]]\n",
        "\n",
        "## limpiar urls diplicadas\n",
        "urls = [url for url in urls if url is not None]\n",
        "urls = [*set(urls)]\n",
        "\n",
        "# parte 3\n",
        "## extraer detalle\n",
        "print(\"---------------Inicia parte 3: extracción del detalle----------------\")\n",
        "urls = [url for url in urls if url is not None]\n",
        "\n",
        "## datos\n",
        "partes = 10000\n",
        "inicio = 1\n",
        "\n",
        "## crear particiones\n",
        "parts = particiones(len(urls), part=partes)\n",
        "\n",
        "for part in range(inicio, parts):\n",
        "\n",
        "    ## partir el dataframe\n",
        "    rango_i, rango_f = partes*part, partes*(part+1)\n",
        "    rango_f = rango_f if rango_f < len(urls) else len(urls)\n",
        "    urls_partidas = urls[rango_i:rango_f]\n",
        "    \n",
        "    ## crear un dataframe para alojar el detalle\n",
        "    inm_detalles = pd.DataFrame()\n",
        "    print(\"Inicio de extracción de información, parte {}\".format(part+1))\n",
        "\n",
        "    for url_num, url in enumerate(urls_partidas):\n",
        "        \n",
        "        # mostrar avance\n",
        "        texto = (\n",
        "            \"\\rProcesado: \" + str(url_num+1) + \"/\" + str(len(urls_partidas)) + \n",
        "            \" \" + str(100*(url_num+1)/len(urls_partidas)) + \"%\"\n",
        "        )\n",
        "        if url_num + 1 == len(urls_partidas):\n",
        "            texto += \"\\r\"\n",
        "        print(texto, end=\"\")  \n",
        "\n",
        "        try:\n",
        "            inm_detalles = pd.concat([inm_detalles, extraer_detalle_mc(url)])\n",
        "        except:\n",
        "            time.sleep(1)\n",
        "            continue\n",
        "        ## archivo de detalle\n",
        "    inms_file = \"./metrocuadrado-{}-parte{}.xlsx\".format(fecha,part)\n",
        "\n",
        "    ## exportar a excel\n",
        "    with pd.ExcelWriter(inms_file, options={'strings_to_urls': False}) as file:\n",
        "        inm_detalles.to_excel(file, index=False)\n",
        "    \n",
        "    ## avisar\n",
        "    print(\"Se extrajo la parte {} de {}\".format(part+1, parts))\n",
        "\n",
        "# parte 4\n",
        "## fin\n",
        "print(\"---------------Inicia parte 4: finalización---------------\")\n",
        "driver.close()\n",
        "print(\"Extracción finalizada a las {}\".format(dt.datetime.now()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "#-------------------------------------------------------------------------\n",
            "# Author: Juan Pablo Trujillo Alviz \n",
            "# github: juapatral\n",
            "# CD: 2021-04-07 \n",
            "# LUD: 2021-04-07\n",
            "# Description: webscrapping of metrocuadrado website\n",
            "# Use: \n",
            "#\n",
            "#\n",
            "# v1\n",
            "# Modification:\n",
            "# Description:\n",
            "#-------------------------------------------------------------------------\n",
            "\n",
            "---------------Inicia parte 1: configuración del chrome---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:90: DeprecationWarning: use options instead of chrome_options\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inicia la extracción a las: 2021-04-15 12:54:21.681528\n",
            "---------------Inicia parte 2: búsqueda de inmuebles---------------\n",
            "---------------Inicia parte 3: extracción del detalle----------------\n",
            "Inicio de extracción de información, parte 2\n",
            "Procesado: 9321/10000 93.21%"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}