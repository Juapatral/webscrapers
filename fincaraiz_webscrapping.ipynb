{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fincaraiz_webscrapping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPS3kQbx90k6KP4LLyTik+M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juapatral/webscrapers/blob/main/fincaraiz_webscrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z42NAbq23GyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98cb9f5d-14d3-474c-ebd2-872345c8ab30"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "print(\n",
        "\"\"\"\n",
        "#-------------------------------------------------------------------------\n",
        "# Author: Juan Pablo Trujillo Alviz \n",
        "# github: juapatral\n",
        "# CD: 2020-09-24 \n",
        "# LUD: 2020-09-24\n",
        "# Description: webscrapping of fincaraiz website\n",
        "# Use: modify weblink and file_name\n",
        "#\n",
        "#\n",
        "# v3\n",
        "# Modification:\n",
        "# Description:\n",
        "#-------------------------------------------------------------------------\n",
        "\"\"\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "#-------------------------------------------------------------------------\n",
            "# Author: Juan Pablo Trujillo Alviz \n",
            "# github: juapatral\n",
            "# CD: 2020-09-24 \n",
            "# LUD: 2020-09-24\n",
            "# Description: webscrapping of fincaraiz website\n",
            "# Use: modify weblink and file_name\n",
            "#\n",
            "#\n",
            "# v3\n",
            "# Modification:\n",
            "# Description:\n",
            "#-------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qROTQHJS0gYO"
      },
      "source": [
        "### webscrapping fincaraiz\n",
        "\n",
        "## importar librerias\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import datetime as dt\n",
        "import glob\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib3\n",
        "import time\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOtAjrbZ0g5K"
      },
      "source": [
        "## crear funciones propias\n",
        "\n",
        "def diccionario(nuevo_texto):\n",
        "    \"\"\"Crea un diccionario con una lista de tuplas llave valor, \n",
        "    si no tiene valor queda nulo\n",
        "    \"\"\"\n",
        "    \n",
        "    # crear diciconario vacio\n",
        "    dicc = {}\n",
        "\n",
        "    # iterar sobre la lista\n",
        "    for tupla in nuevo_texto:\n",
        "        # crear variable\n",
        "        try:\n",
        "            tupla2 = tupla[1] if tupla[1] != \"\" else None\n",
        "            dicc[tupla[0]] = tupla2\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return dicc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmyO53AS0q8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b083d784-9468-49ec-a9f3-2ed2572f6a01"
      },
      "source": [
        "## parametros fijos\n",
        "\n",
        "# date time\n",
        "now = dt.datetime.now()\n",
        "\n",
        "# conteo de paginas y maximo numero de paginas a buscar\n",
        "number_page, max_number_page = 1, 7000\n",
        "\n",
        "# pagina inicial a buscar\n",
        "weblink = \"https://www.fincaraiz.com.co/Vivienda/venta/\"\n",
        "path = \"\" # cambiar si quieren alguna ruta de su drive, por ejemplo\n",
        "#ubicacion = weblink.split(\"/\")[5].lower()\n",
        "tipo_inmueble = weblink.split(\"/\")[3].lower()\n",
        "tipo_negocio = weblink.split(\"/\")[4].lower()\n",
        "fecha = now.year * 10000 + now.month * 100 + now.day\n",
        "file_name = (\n",
        "    #\"fincaraiz-{}-{}_{}-{}.csv\"\n",
        "    \"fincaraiz-{}-{}_{}.csv\"\n",
        "    #.format(ubicacion, tipo_inmueble, tipo_negocio, fecha)\n",
        "    .format(tipo_inmueble, tipo_negocio, fecha)\n",
        ")\n",
        "\n",
        "# desahilitar advertencias\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "# show time\n",
        "print(\"EXTRACCIÓN DE: {}\".format(file_name))\n",
        "print(\"Empieza la extracción a las {}\".format(now))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EXTRACCIÓN DE: fincaraiz-vivienda-venta_20210413.csv\n",
            "Empieza la extracción a las 2021-04-13 00:57:08.779847\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxvlplaR0tvD"
      },
      "source": [
        "### ejecucion\n",
        "\n",
        "## parte 1 extraer urls de las paginas\n",
        "\n",
        "# encontrar archivo de urls\n",
        "urls_df_list = glob.glob(\"./*urls*\")\n",
        "\n",
        "\n",
        "if urls_df_list == []:\n",
        "\n",
        "    # listas vacias de url\n",
        "    urls = []\n",
        "\n",
        "    # crear loop que vaya por cada pagina solicitada\n",
        "    while number_page <= max_number_page:\n",
        "        number_page += 1\n",
        "\n",
        "        # hacer el request\n",
        "        try:\n",
        "            r = requests.get(weblink, timeout=(15, 15), verify=False)\n",
        "        except:\n",
        "            break\n",
        "\n",
        "        # si la conexion no es exitosa\n",
        "        if r.status_code != 200:\n",
        "            break\n",
        "\n",
        "        # convertir el html en texto\n",
        "        content = r.content\n",
        "        soup = BeautifulSoup(content, \"html.parser\")\n",
        "\n",
        "\n",
        "        # encontrar pagina siguiente\n",
        "        try:\n",
        "            weblink = soup.find(\"link\", {\"rel\":\"next\"})[\"href\"]    \n",
        "        except:\n",
        "            break\n",
        "\n",
        "        # encontrar bloques de ventas\n",
        "        divAdverts = soup.find(\"div\", {\"id\":\"divAdverts\"})\n",
        "\n",
        "        # encontrar cada bloque de venta\n",
        "        ul = divAdverts.find_all(\"ul\")\n",
        "\n",
        "        # extraer las urls de cada bloque\n",
        "        for u in ul:\n",
        "            ads = u.find(\"div\", {\"class\":\"AdvertisingContainer\"})\n",
        "\n",
        "            # si es un anuncio se omite\n",
        "            if ads is not None:\n",
        "                continue\n",
        "            \n",
        "            # extraer las url\n",
        "            try:\n",
        "                url = u.find_all(\"a\", href=True)\n",
        "                full_url = \"https://www.fincaraiz.com.co\" + str(url[0]['href'])\n",
        "\n",
        "            except:\n",
        "                full_url = None\n",
        "            \n",
        "            # adicionar al listado de urls\n",
        "            urls.append(full_url)\n",
        "\n",
        "    # informar\n",
        "    print(\"Se extrajo {} páginas de información\".format(number_page))\n",
        "\n",
        "    # limpiar urls nulas \n",
        "    urls = [url for url in urls if url is not None]\n",
        "    urls = [*set(urls)]\n",
        "    # informar\n",
        "    print(\"Y se encontraron {} urls de inmuebles\".format(len(urls)))\n",
        "\n",
        "    # crear dataframe\n",
        "    urls_df = pd.DataFrame(urls)\n",
        "    urls_df.to_csv(path + \"urls_\" + file_name, index=False)\n",
        "    files.download(path + \"urls_\" + file_name)\n",
        "else:\n",
        "    urls_df = pd.read_csv(urls_df_list[0])\n",
        "    urls = [*urls_df[\"0\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqAj0c2nuei1"
      },
      "source": [
        "## parte 2 extraer informacion de las urls\n",
        "\n",
        "# funcion de particiones\n",
        "def particiones(numb, part=50000):\n",
        "    return numb//part + (1 if numb%part != 0 else 0)\n",
        "\n",
        "def extraer_info_urls(urls, iteracion=0):\n",
        "\n",
        "    # crear lista diccionarios vacia\n",
        "    lista_dicc = []\n",
        "    \n",
        "    # request por cada url\n",
        "    for i, url in enumerate(urls):\n",
        "\n",
        "        # mostrar avance\n",
        "        texto = (\n",
        "            \"\\rProcesado: \" + str(i+1) + \"/\" + str(len(urls)) + \" \" + \n",
        "            str(100*(i+1)/len(urls)) + \"%\"\n",
        "        )\n",
        "        if i + 1 == len(urls):\n",
        "            texto += \"\\r\"\n",
        "        print(texto, end=\"\")    \n",
        "        time.sleep(0.1)\n",
        "        \n",
        "        # hacer el request\n",
        "        try:\n",
        "            inm_r = requests.get(url, verify=False)\n",
        "        except:\n",
        "            continue\n",
        "        \n",
        "        if inm_r.status_code != 200:\n",
        "            continue\n",
        "            \n",
        "        inm_content = inm_r.content\n",
        "        inm_soup = BeautifulSoup(inm_content, \"html.parser\")    \n",
        "\n",
        "        # encontrar json con informacion\n",
        "        scripts = inm_soup.find_all(\"script\", {\"type\":\"text/javascript\"})\n",
        "        texto = []\n",
        "\n",
        "        # si el request falla\n",
        "        if scripts == []:\n",
        "            continue\n",
        "\n",
        "        # extraer el json\n",
        "        for s in scripts:\n",
        "            texto = re.findall(\"var sfAdvert = {(.*)}\", s.text)\n",
        "            if texto != []:\n",
        "                break\n",
        "        \n",
        "        # si no encuentra informacion del inmueble\n",
        "        if texto == []:\n",
        "            continue\n",
        "\n",
        "        # convertir el json en un diccionario\n",
        "        nuevo_texto = texto[0].split(\"\\\", \")\n",
        "        nuevo_texto2 = [t.replace(\"\\\"\",\"\").split(\" : \") for t in nuevo_texto]\n",
        "        dicc = diccionario(nuevo_texto2)\n",
        "\n",
        "        # adicionar a la lista de diccionarios\n",
        "        lista_dicc.append(dicc)\n",
        "\n",
        "    ## parte 3 creacion del dataframe\n",
        "\n",
        "    # convertir cada diccionario en una lista de dataframe\n",
        "    lista_df = [pd.DataFrame(dic, index=[0]) for dic in lista_dicc]\n",
        "\n",
        "    # concatenar dataframe\n",
        "    df = pd.concat(lista_df).reset_index(drop=True)\n",
        "\n",
        "    # crear columnas faltantes\n",
        "    df[\"extraction_year\"] = now.year\n",
        "    df[\"extraction_month\"] = now.month\n",
        "    df[\"extraction_day\"] = now.day\n",
        "\n",
        "    # eliminar duplicados\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    # guardar dataframe\n",
        "    df.to_csv(path + file_name.replace(\".csv\", \"-parte{}.csv\").format(iteracion), index=False)\n",
        "    files.download(path + file_name.replace(\".csv\", \"-parte{}.csv\").format(iteracion))\n",
        "\n",
        "    # tiempo\n",
        "    print(\"Se demoró (HH:MM:SS): \", dt.datetime.now()-now)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "m8lTPP4rzEJP",
        "outputId": "fb9fa677-2eaf-480d-a90e-9b98563f915c"
      },
      "source": [
        "# limpiar urls nulas \n",
        "urls = [url for url in urls if url is not None]\n",
        "\n",
        "# datos\n",
        "partes = 30000\n",
        "inicio = 6\n",
        "\n",
        "# crear particiones\n",
        "parts = particiones(len(urls), part=partes)\n",
        "print(\"Se extraerán {} particiones\".format(parts))\n",
        "\n",
        "for part in range(inicio, parts):\n",
        "    print(\"Inicia la partición {}\".format(part+1))\n",
        "    rango_i, rango_f = partes*part, partes*(part+1)\n",
        "    rango_f = rango_f if rango_f < len(urls) else len(urls)\n",
        "    urls_partidas = urls[rango_i:rango_f]\n",
        "    extraer_info_urls(urls_partidas, part)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Se extraerán 7 particiones\n",
            "Inicia la partición 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c518e8bd-6709-4578-9264-559ee5d234c2\", \"fincaraiz-vivienda-venta_20210413-parte6.csv\", 16213050)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Se demoró (HH:MM:SS):  2:41:42.772703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wdbo2zh1LxC"
      },
      "source": [
        "\n",
        "## parte 3 creacion del dataframe\n",
        "\n",
        "# convertir cada diccionario en una lista de dataframe\n",
        "lista_df = [pd.DataFrame(dic, index=[0]) for dic in lista_dicc]\n",
        "\n",
        "# concatenar dataframe\n",
        "df = pd.concat(lista_df).reset_index(drop=True)\n",
        "\n",
        "# crear columnas faltantes\n",
        "df[\"extraction_year\"] = now.year\n",
        "df[\"extraction_month\"] = now.month\n",
        "df[\"extraction_day\"] = now.day\n",
        "\n",
        "# eliminar duplicados\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# guardar dataframe\n",
        "df.to_csv(path + file_name, index=False)\n",
        "files.download(path + file_name) \n",
        "\n",
        "# tiempo\n",
        "print(\"Se demoró (HH:MM:SS): \", dt.datetime.now()-now)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCSINqVjHUFi"
      },
      "source": [
        " urls = pd.read_csv(\"/content/urls_fincaraiz-cundinamarca-vivienda_venta-20200930.csv\")\n",
        "urls = [*urls[\"0\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9_mtuo0y5i-"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"fincaraiz_webscrapping.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1HtkSUSD5gBVdPHUEOgMo6si9yihmGCT9\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "#-------------------------------------------------------------------------\n",
        "# Author: Juan Pablo Trujillo Alviz \n",
        "# github: juapatral\n",
        "# CD: 2020-09-24 \n",
        "# LUD: 2020-09-24\n",
        "# Description: webscrapping of fincaraiz website\n",
        "#\n",
        "#\n",
        "# example of use:\n",
        "#>>>ciudad = webscrapping(\n",
        "#               \"path_of_directory\", \n",
        "#               \"www.fincaraiz.com.co/Vivienda/venta/medellin\"\n",
        "#)\n",
        "#-------------------------------------------------------------------------\n",
        "\"\"\"\n",
        "\n",
        "### webscrapping fincaraiz\n",
        "\n",
        "## importar librerias\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import datetime as dt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib3\n",
        "import time\n",
        "import os\n",
        "\n",
        "## crear funciones propias\n",
        "\n",
        "def diccionario(nuevo_texto):\n",
        "    \"\"\"Crea un diccionario con una lista de tuplas llave valor, \n",
        "    si no tiene valor queda nulo\n",
        "    \"\"\"\n",
        "    \n",
        "    # crear diciconario vacio\n",
        "    dicc = {}\n",
        "\n",
        "    # iterar sobre la lista\n",
        "    for tupla in nuevo_texto:\n",
        "        # crear variable\n",
        "        tupla2 = tupla[1] if tupla[1]!=\"\" else None\n",
        "        dicc[tupla[0]] = tupla2\n",
        "    return dicc\n",
        "\n",
        "## parametros fijos\n",
        "\n",
        "def webscrapping(data_path, weblink):\n",
        "    \n",
        "    # date time\n",
        "    now = dt.datetime.now()\n",
        "\n",
        "    # conteo de paginas y maximo numero de paginas a buscar\n",
        "    number_page, max_number_page = 1, 10\n",
        "\n",
        "    # listas vacias de url y diccionario de datos\n",
        "    urls, lista_dicc = [], []\n",
        "\n",
        "    # pagina inicial a buscar\n",
        "    #weblink = \"https://www.fincaraiz.com.co/arriendos/\"\n",
        "\n",
        "    # nombrar el weblink\n",
        "    ubicacion = weblink.split(\"/\")[5].lower()\n",
        "    tipo_inmueble = weblink.split(\"/\")[3].lower()\n",
        "    tipo_negocio = weblink.split(\"/\")[4].lower()\n",
        "    fecha = now.year * 10000 + now.month * 100 + now.day\n",
        "    file_name = (\n",
        "        \"fincaraiz-{}-{}_{}-{}.csv\"\n",
        "        .format(ubicacion, tipo_inmueble, tipo_negocio, fecha)\n",
        "    )\n",
        "\n",
        "    # desahilitar advertencias\n",
        "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "    ### ejecucion\n",
        "\n",
        "    ## parte 1 extraer urls de las paginas\n",
        "\n",
        "    # crear loop que vaya por cada pagina solicitada\n",
        "    while number_page <= max_number_page:\n",
        "        number_page += 1\n",
        "\n",
        "        # hacer el request\n",
        "        r = requests.get(weblink, verify=False)\n",
        "        content = r.content\n",
        "        soup = BeautifulSoup(content, \"html.parser\")\n",
        "\n",
        "\n",
        "        # encontrar pagina siguiente\n",
        "        weblink = soup.find(\"link\", {\"rel\":\"next\"})[\"href\"]    \n",
        "\n",
        "        # encontrar bloques de ventas\n",
        "        divAdverts = soup.find(\"div\", {\"id\":\"divAdverts\"})\n",
        "\n",
        "        # encontrar cada bloque de venta\n",
        "        ul = divAdverts.find_all(\"ul\")\n",
        "\n",
        "        # extraer las urls de cada bloque\n",
        "        for u in ul:\n",
        "            ads = u.find(\"div\", {\"class\":\"AdvertisingContainer\"})\n",
        "\n",
        "            # si es un anuncio se omite\n",
        "            if ads is not None:\n",
        "                continue\n",
        "            \n",
        "            # extraer las url\n",
        "            try:\n",
        "                url = u.find_all(\"a\", href=True)\n",
        "                full_url = \"https://www.fincaraiz.com.co\" + str(url[0]['href'])\n",
        "\n",
        "            except:\n",
        "                full_url = None\n",
        "            \n",
        "            # adicionar al listado de urls\n",
        "            full_url = full_url if full_url.count(\"http\") <= 1 else None\n",
        "            urls.append(full_url)\n",
        "\n",
        "    ## parte 2 extraer informacion de las urls\n",
        "    \n",
        "    # limpiar urls nulas \n",
        "    urls = [url for url in urls if url is not None]\n",
        "\n",
        "    # request por cada url\n",
        "    for i, url in enumerate(urls):\n",
        "\n",
        "        # mostrar avance\n",
        "        texto = (\n",
        "            \"\\rProcesado: \" + str(i+1) + \"/\" + str(len(urls)) + \" \" + \n",
        "            str(100*(i+1)/len(urls)) + \"%\"\n",
        "        )\n",
        "        if i + 1 == len(urls):\n",
        "            texto += \"\\r\"\n",
        "        print(texto, end=\"\")    \n",
        "        time.sleep(0.1)\n",
        "        \n",
        "        # hacer el request\n",
        "        inm_r = requests.get(url, verify=False)\n",
        "        inm_content = inm_r.content\n",
        "        inm_soup = BeautifulSoup(inm_content, \"html.parser\")    \n",
        "\n",
        "        # encontrar json con informacion\n",
        "        scripts = inm_soup.find_all(\"script\", {\"type\":\"text/javascript\"})\n",
        "        texto = []\n",
        "\n",
        "        # si el request falla\n",
        "        if scripts == []:\n",
        "            continue\n",
        "\n",
        "        # extraer el json\n",
        "        for s in scripts:\n",
        "            texto = re.findall(\"var sfAdvert = {(.*)}\", s.text)\n",
        "            if texto != []:\n",
        "                break\n",
        "        \n",
        "        # si no encuentra informacion del inmueble\n",
        "        if texto == []:\n",
        "            continue\n",
        "\n",
        "        # convertir el json en un diccionario\n",
        "        nuevo_texto = texto[0].split(\"\\\", \")\n",
        "        nuevo_texto2 = [t.replace(\"\\\"\",\"\").split(\" : \") for t in nuevo_texto]\n",
        "        dicc = diccionario(nuevo_texto2)\n",
        "\n",
        "        # adicionar a la lista de diccionarios\n",
        "        lista_dicc.append(dicc)\n",
        "\n",
        "    ## parte 3 creacion del dataframe\n",
        "\n",
        "    # convertir cada diccionario en una lista de dataframe\n",
        "    lista_df = [pd.DataFrame(dic, index=[0]) for dic in lista_dicc]\n",
        "\n",
        "    # concatenar dataframe\n",
        "    df = pd.concat(lista_df).reset_index(drop=True)\n",
        "\n",
        "    # crear columnas faltantes\n",
        "    df[\"extraction_year\"] = now.year\n",
        "    df[\"extraction_month\"] = now.month\n",
        "    df[\"extraction_day\"] = now.day\n",
        "\n",
        "    # eliminar duplicados\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    # tiempo\n",
        "    print(\"Se demoró (HH:MM:SS): \", dt.datetime.now()-now)\n",
        "\n",
        "    # guardar dataframe\n",
        "    df.to_csv(data_path + file_name, index=False)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZl1kgPCy6BQ"
      },
      "source": [
        "data_path = \"\"\n",
        "weblink = \"https://www.fincaraiz.com.co/Vivienda/venta/fusagasuga/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMiJqt98zEMv"
      },
      "source": [
        "site = \"https://www.fincaraiz.com.co/proyecto-parque-la-colina--coburgo/fusagasuga/proyecto-nuevo-det-1140183.aspx?itemid=5245095&pag=1&pos=3&IsProjectWithOutProduct=True\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQyuccjwztUk"
      },
      "source": [
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4vl557-zui-"
      },
      "source": [
        "r = requests.get(site)\n",
        "content = r.content\n",
        "soup = BeautifulSoup(content, \"html.parser\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkWxK7E0zvyO"
      },
      "source": [
        "soup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm_6I9Kfz4Au"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}